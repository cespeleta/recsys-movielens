{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torchmetrics import MeanSquaredError\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dynaconf import Dynaconf\n",
    "import lightning as L\n",
    "\n",
    "from src.models.mf_with_bias import MatrixFactorizationWithBias\n",
    "from src.lit_models.base import LightningModel\n",
    "\n",
    "from src.datasets.movielens import MovielensDataModule"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config with dynaconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Dynaconf(root_path=\"configs\", settings_files=[\"config_mf.yaml\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model inputs from the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users=943, n_items=1625, n_factors=128\n"
     ]
    }
   ],
   "source": [
    "n_users = cfg.model.pytorch_model.init_args.n_users\n",
    "n_items = cfg.model.pytorch_model.init_args.n_items\n",
    "n_factors = cfg.model.pytorch_model.init_args.n_factors\n",
    "print(f\"{n_users=}, {n_items=}, {n_factors=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MatrixFactorizationWithBias(\n",
       "  (user_emb): Embedding(943, 128)\n",
       "  (user_bias): Embedding(943, 1)\n",
       "  (item_emb): Embedding(1625, 128)\n",
       "  (item_bias): Embedding(1625, 1)\n",
       ")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_model = MatrixFactorizationWithBias(n_users, n_items, 128)\n",
    "pytorch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/cespeleta/Projects/recsys-movielens/lightning_logs/embedding_dim/version_1/checkpoints/best_model.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m checkpoint_file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlightning_logs/embedding_dim/version_1/checkpoints/best_model.ckpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39m# load weights\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m LightningModel\u001b[39m.\u001b[39;49mload_from_checkpoint(\n\u001b[1;32m      5\u001b[0m     checkpoint_path\u001b[39m=\u001b[39;49mcheckpoint_file, pytorch_model\u001b[39m=\u001b[39;49mpytorch_model\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/recsys-movielens/.venv/lib/python3.11/site-packages/lightning/pytorch/core/module.py:1531\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   1452\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_from_checkpoint\u001b[39m(\n\u001b[1;32m   1453\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1458\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1459\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Self:\n\u001b[1;32m   1460\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1461\u001b[0m \u001b[39m    Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\u001b[39;00m\n\u001b[1;32m   1462\u001b[0m \u001b[39m    it stores the arguments passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1529\u001b[0m \u001b[39m        y_hat = pretrained_model(x)\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1531\u001b[0m     loaded \u001b[39m=\u001b[39m _load_from_checkpoint(\n\u001b[1;32m   1532\u001b[0m         \u001b[39mcls\u001b[39;49m,\n\u001b[1;32m   1533\u001b[0m         checkpoint_path,\n\u001b[1;32m   1534\u001b[0m         map_location,\n\u001b[1;32m   1535\u001b[0m         hparams_file,\n\u001b[1;32m   1536\u001b[0m         strict,\n\u001b[1;32m   1537\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1538\u001b[0m     )\n\u001b[1;32m   1539\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[0;32m~/Projects/recsys-movielens/.venv/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:60\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_from_checkpoint\u001b[39m(\n\u001b[1;32m     52\u001b[0m     \u001b[39mcls\u001b[39m: Union[Type[\u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m], Type[\u001b[39m\"\u001b[39m\u001b[39mpl.LightningDataModule\u001b[39m\u001b[39m\"\u001b[39m]],\n\u001b[1;32m     53\u001b[0m     checkpoint_path: Union[_PATH, IO],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m     58\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[\u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpl.LightningDataModule\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     59\u001b[0m     \u001b[39mwith\u001b[39;00m pl_legacy_patch():\n\u001b[0;32m---> 60\u001b[0m         checkpoint \u001b[39m=\u001b[39m pl_load(checkpoint_path, map_location\u001b[39m=\u001b[39;49mmap_location)\n\u001b[1;32m     62\u001b[0m     \u001b[39m# convert legacy checkpoints to the new format\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     checkpoint \u001b[39m=\u001b[39m _pl_migrate_checkpoint(\n\u001b[1;32m     64\u001b[0m         checkpoint, checkpoint_path\u001b[39m=\u001b[39m(checkpoint_path \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(checkpoint_path, (\u001b[39mstr\u001b[39m, Path)) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     65\u001b[0m     )\n",
      "File \u001b[0;32m~/Projects/recsys-movielens/.venv/lib/python3.11/site-packages/lightning/fabric/utilities/cloud_io.py:50\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mhub\u001b[39m.\u001b[39mload_state_dict_from_url(\n\u001b[1;32m     46\u001b[0m         \u001b[39mstr\u001b[39m(path_or_url),\n\u001b[1;32m     47\u001b[0m         map_location\u001b[39m=\u001b[39mmap_location,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     )\n\u001b[1;32m     49\u001b[0m fs \u001b[39m=\u001b[39m get_filesystem(path_or_url)\n\u001b[0;32m---> 50\u001b[0m \u001b[39mwith\u001b[39;00m fs\u001b[39m.\u001b[39;49mopen(path_or_url, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     51\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mload(f, map_location\u001b[39m=\u001b[39mmap_location)\n",
      "File \u001b[0;32m~/Projects/recsys-movielens/.venv/lib/python3.11/site-packages/fsspec/spec.py:1199\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1198\u001b[0m     ac \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mautocommit\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_intrans)\n\u001b[0;32m-> 1199\u001b[0m     f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(\n\u001b[1;32m   1200\u001b[0m         path,\n\u001b[1;32m   1201\u001b[0m         mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   1202\u001b[0m         block_size\u001b[39m=\u001b[39;49mblock_size,\n\u001b[1;32m   1203\u001b[0m         autocommit\u001b[39m=\u001b[39;49mac,\n\u001b[1;32m   1204\u001b[0m         cache_options\u001b[39m=\u001b[39;49mcache_options,\n\u001b[1;32m   1205\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1206\u001b[0m     )\n\u001b[1;32m   1207\u001b[0m     \u001b[39mif\u001b[39;00m compression \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1208\u001b[0m         \u001b[39mfrom\u001b[39;00m \u001b[39mfsspec\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompression\u001b[39;00m \u001b[39mimport\u001b[39;00m compr\n",
      "File \u001b[0;32m~/Projects/recsys-movielens/.venv/lib/python3.11/site-packages/fsspec/implementations/local.py:183\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[0;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_mkdir \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmakedirs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent(path), exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m LocalFileOpener(path, mode, fs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/recsys-movielens/.venv/lib/python3.11/site-packages/fsspec/implementations/local.py:314\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[0;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompression \u001b[39m=\u001b[39m get_compression(path, compression)\n\u001b[1;32m    313\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocksize \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[0;32m--> 314\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open()\n",
      "File \u001b[0;32m~/Projects/recsys-movielens/.venv/lib/python3.11/site-packages/fsspec/implementations/local.py:319\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf\u001b[39m.\u001b[39mclosed:\n\u001b[1;32m    318\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautocommit \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode:\n\u001b[0;32m--> 319\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath, mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode)\n\u001b[1;32m    320\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompression:\n\u001b[1;32m    321\u001b[0m             compress \u001b[39m=\u001b[39m compr[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompression]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/cespeleta/Projects/recsys-movielens/lightning_logs/embedding_dim/version_1/checkpoints/best_model.ckpt'"
     ]
    }
   ],
   "source": [
    "checkpoint_file = \"lightning_logs/embedding_dim/version_1/checkpoints/best_model.ckpt\"\n",
    "\n",
    "# load weights\n",
    "model = LightningModel.load_from_checkpoint(\n",
    "    checkpoint_path=checkpoint_file, pytorch_model=pytorch_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = model.device\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = MovielensDataModule(dataset=\"ml-100k\", target=\"rating\", batch_size=32)\n",
    "dm.setup(stage=\"test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on new data\n",
    "\n",
    "Here we are going to use test set as a new data but, of course, we could use any new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = dm.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first batch of data\n",
    "for batch_data in test_dataloader:\n",
    "    users = batch_data[\"user\"].to(device)\n",
    "    items = batch_data[\"item\"].to(device)\n",
    "    ratings = batch_data[\"rating\"].to(device)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch\n",
    "\n",
    "We can make our predictions with plain Pytorch our using the Lighning Trainer.\n",
    "\n",
    "Docs: https://lightning.ai/docs/pytorch/stable/deploy/production_intermediate.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    y_hat = model(users, items)  #* (5.5 - 1) + 1\n",
    "\n",
    "y_hat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is equivalent to call the `forward` of the pytorch model directly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    print(model.pytorch_model(users, items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse = MeanSquaredError().to(device)\n",
    "# pred_list = []\n",
    "# for batch_data in test_dataloader:\n",
    "#     users = batch_data[\"user\"].to(device)\n",
    "#     items = batch_data[\"item\"].to(device)\n",
    "#     ratings = batch_data[\"rating\"].to(device)\n",
    "#     with torch.inference_mode():\n",
    "#         y_hat = model.predict_step(batch_data) * (5.5 - 1) + 1\n",
    "#         pred_list.append(y_hat.cpu().detach().numpy().squeeze())\n",
    "    \n",
    "#     mse(y_hat, ratings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Lightning Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(enable_checkpointing=False)\n",
    "batched_predictions = trainer.predict(model, dataloaders=[test_dataloader])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, show first batch fo predictions. Why are they different from the pure pytorch predictions? That's because in the LightningModule predictions are transformed using `Sigmoid` to normalize the oputpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_predictions[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all predictions in a single vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.cat(batched_predictions)\n",
    "predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get true ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = torch.cat([batch_data[\"user\"] for batch_data in test_dataloader], dim=0)\n",
    "items = torch.cat([batch_data[\"item\"] for batch_data in test_dataloader], dim=0)\n",
    "ratings = torch.cat([batch_data[\"rating\"] for batch_data in test_dataloader], dim=0)\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_predictions(x, range=(1, 5.5)):\n",
    "    min_y, max_y = range\n",
    "    return x * (max_y - min_y) + min_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = MeanSquaredError()\n",
    "rmse = MeanSquaredError(squared=False)\n",
    "\n",
    "scaled_predictions = scale_predictions(predictions)\n",
    "\n",
    "test_mse = mse(scaled_predictions, ratings)\n",
    "test_rmse = rmse(scaled_predictions, ratings)\n",
    "\n",
    "print(f\"Test MSE: {test_mse:.4f}\")\n",
    "print(f\"Test RMSE: {torch.sqrt(test_rmse):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mae = mse.compute()\n",
    "print(f\"Test MSE: {test_mse:.4f}\")\n",
    "print(f\"Test RMSE: {torch.sqrt(test_mse):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mappng\n",
    "import joblib\n",
    "\n",
    "\n",
    "item2int = joblib.load(\"output/encoders/ml-100k/title_encoder.joblib\")\n",
    "int2item = {v: k for k, v in item2int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_hat):\n",
    "    return np.sqrt(np.mean((y_true - y_hat) ** 2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veamos las diferencias...\n",
    "data = torch.vstack([users, items, ratings, scaled_predictions]).detach().cpu().numpy().T\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame.from_records(data, columns=[\"users_enc\", \"items_enc\", \"ratings\", \"predictions\"])\n",
    "df.insert(loc=0, column=\"title\", value=df.items_enc.map(int2item))\n",
    "df[\"error\"] = abs(df.ratings - df.predictions)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_df = df.groupby([\"title\"]).agg(error=(\"error\", \"mean\"), count=(\"title\", \"count\"))\n",
    "errors_df.sort_values(by=\"error\", ascending=True)\n",
    "\n",
    "# Movies with lower error\n",
    "errors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movies with biggest errors\n",
    "errors_df.sort_values(by=\"error\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
